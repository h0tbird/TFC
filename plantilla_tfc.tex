\documentclass[a4paper,12pt,spanish,final]{epsc_tfc_pfc}

%-----------
% Preamble:
%-----------

\usepackage[spanish,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath,amsfonts}
\usepackage{array}
\usepackage{listings}
\usepackage{color}
\usepackage{eurosym}
\usepackage[table,xcdraw]{xcolor}
\usepackage{wrapfig,booktabs}
\usepackage[justification=centering]{caption}
\usepackage{wrapfig}
\usepackage{enumitem}

%---------------------------------
% Code snippets style definition:
%---------------------------------

\definecolor{softgray}{RGB}{245,245,245}
\definecolor{softred}{RGB}{222,55,55}
\lstdefinestyle{dnsmasq}{
  basicstyle=\scriptsize,
  frame=single,
  backgroundcolor=\color{softgray},
  escapeinside={(*@}{@*)}
}

%-------------------------------
% Content is about to commence:
%-------------------------------

\begin{document}
\pagestyle{empty}
\selectlanguage{spanish}
\portada{}

%----------
% Resumen:
%----------

\begin{resum}{1cm}
En este proyecto se intenta dar solución a dos problemáticas relacionadas con la escalabilidad de grandes infraestructuras de computación: la utilización ineficiente de los recursos del centro de datos y el incremento de la complejidad de la gestión administrativa.

No hay que retroceder mucho en el tiempo para constatar que tanto las estrategias de gestión de configuraciones como las de asignación de recursos en flotas de servidores han evolucionado muy rápidamente. El servidor deja de ser el centro de gravitación y pierde protagonismo a la vez que el centro de datos se consolida como la nueva capa de abstracción. Sobre ésta se ejecutan todo tipo de cargas computacionales.\\

En este proyecto se presentan dos plataformas en simbiosis, la plataforma de arranque o \emph{bootstrapping} y la plataforma de explotación. La primera sirve para desplegar y gestionar la siguiente que, a su vez, unifica los recursos del centro de datos y los asigna de forma dinámica en función de la demanda instantánea. El proyecto tiene componentes de diseño, desarrollo e integración.\\

La plataforma de explotación es la más extensa en cuanto al número de componentes que la integran. Sin embargo, solamente ha requerido de un esfuerzo de integración. En el proyecto se detallan los distintos elementos que la conforman, sus funciones y cómo se interrelacionan entre sí.\\

La plataforma de arranque, siendo la más pequeña, tiene los mayores componentes de diseño y desarrollo. En el proyecto se detallan los distintos elementos que la conforman, sus funciones, su proceso de construción y el flujo completo de interacciones.\\

El proyecto termina con una guía de instalación de la plataforma de arranque para que el lector pueda desplegar en su propia infraestructura una plataforma de explotación.
\end{resum}

\selectlanguage{english}

\begin{overview}{1cm}
The aim of this project is to provide a solution to two scalability issues that arise in big computation infraestructures: Inefficient resources utilization in the data center and the increase of the management complexity.

Regarding fleets of servers, you don't have to go back far in time to verify that both configuration management and resources allocation polices have evolved very fast. The server is no longer the gravitation center and the data center consolidates as the new abstraction layer where computation happens.\\

This project presents two platforms in simbiosis, the bootstrapping platform and the exploitation platform. The former is used to bootstrapp and manage the latter, which in turn unifies the data center resources and performs dynamic resources allocation based on demand. The project has a balanced effort of design, development and integration phases.\\

The exploitation platform is larger in terms of number of components however, it just required an integration effort. All the building blocks are detailed in the project as well as its functions and its interactions.\\

Despite being the smaller one, the bootstrapping platform requires the higher design and development efforts. All the building blocks are detailed in the project as well as its functions, building recips and full structural and procedural diagrams.\\

The project ends with an installation guide that helps the reader to deploy a bootstrapping platform which in turn can be used to deploy an exploitation platform.
\end{overview}

\selectlanguage{spanish}

%--------------
% Dedicatoria:
%--------------

\begin{dedicatoria}
A mis padres que me enseñaron a querer y a pensar.\\
A Débora por todo su amor y soporte.
\end{dedicatoria}

%-------------------
% Figuras y tablas:
%-------------------

\thispagestyle{empty}
\tableofcontents
\cleardoublepage{}
\thispagestyle{empty}
\listoffigures
\cleardoublepage{}
\thispagestyle{empty}
\listoftables
\cleardoublepage{}
\pagestyle{fancy}

%---------------
% Introducción:
%---------------

\begin{intro}{Introducción}

  Según un estudio publicado por Netcraft [1], en Abril del 2015 habían 850 millones de páginas web alojadas en 5 millones de servidores expuestos en la red (\emph{web-facing}). Muchos de estos servidores, a su vez, no son más que la punta del iceberg de estructuras internas más complejas con servidores no expuestos que realizan otro tipo de funciones. Sería el caso de sitios web de la categoría de Google, Facebook o Twitter.

  Google, por ejemplo, tiene 13 centros de datos distribuidos por todo el planeta [2] con no menos de 75 mil servidores en cada uno de ellos. Los equipos humanos que gestionan estas infraestructuras son proporcionalmente muy reducidos pero, aún y así, consiguen unos niveles de eficiencia muy elevados. ¿Cómo lo hacen?

Los problemas que se manifiestan en grandes infraestructuras también existen en estado latente en infraestructuras medianas y pequeñas. Las empresas pioneras que cruzan por primera vez órdenes de magnitud inexplorados tienen que asumir el coste de desarrollar la tecnología que les permita seguir avanzando. El resto de empresas navegan sobre la estela de la verdadera innovación tecnológica.

Este trabajo se sitúa en dicha estela con la intención de anticipar soluciones de forma preventiva en infraestructuras de tamaño medio. Los dos principales problemas que se han identificado son:

\textbf{Primero: Utilización ineficiente de los recursos del centro de datos}

Varios estudios estiman que la utilización de los recursos de los centros de datos de toda la industria se situa entre el 6\% [3] y el 12\% [4,5]. La principal causa de esta ineficiencia viene dada por la combinación de dos factores:
\begin{enumerate}
  \item El uso de estrategias de asignación estática de recursos.
  \item Los desarrolladores no entienden la dinámica de sus cargas de trabajo y, por lo tanto, no pueden anticipar los requerimientos de recursos físicos que demandan sus bases de código.
\end{enumerate}

En la siguiente figura se representa el desperdicio de recursos que supone la reserva estática (3 nodos) frente a la multiplexación dinámica de recursos (1 nodo).

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.55]{static}
    \caption{Particionado estático vs dinámico.}
\end{figure}

En este trabajo se detalla una plataforma de explotación que es capaz de asignar recursos de forma dinámica a distintas cargas de trabajo simultáneas y, de esta manera, mejorar los niveles de utilización de los recursos en una flota de servidores.

\textbf{Segundo: Incremento de la complejidad de la gestión administrativa}

La complejidad de la gestión administrativa incrementa linealmente \emph{O(n)} con el número de servidores pero lo hace de forma cuadrática \emph{O(n\^{}2)} con la diversidad de sistemas administrados. Es preferible administrar muchos servidores iguales que unos pocos muy diversos.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.27]{complex}
      \caption{Complejidad de la gestión}
\end{figure}

La plataforma de explotación propuesta homogeneiza la instalación de la flota de servidores y desplaza la diversidad de las distintas cargas de trabajo hacia capas superiores de software llamadas \emph{'frameworks'}.

Para dar solución a estas dos problemáticas de escalabilidad, se presentan dos plataformas en simbiosis.

Entendemos por escalabilidad de explotación de un sistema la capacidad del mismo para crecer linealmente y acomodar incrementos de carga computacional. Por otra parte, la escalabilidad de su gestión viene dada por la capacidad de mantener constante la complejidad de su carga administrativa. Veremos que para dar respuesta al primer reto nos centraremos en el ¿qué? y para el segundo en el ¿cómo?

\textbf{¿Qué?} La plataforma propuesta se cimienta sobre varias tecnologías que interactúan entre sí. Las más destacables son: \emph{Linux} (1991), \emph{KVM} (2007), \emph{CoreOS} (2013), \emph{Docker} (2013), \emph{Mesos} (2011) y \emph{Marathon} (2013). Las principales virtudes de ésta son:
\begin{itemize}
  \item Capacidad para acomodar distintos tipos de carga (\emph{frameworks}) simultáneamente, sin necesidad de realizar un particionado estático de los recursos.
  \item Alta densidad de servicios y, por lo tanto, una mejor utilización de los recursos que se traduce en una mayor eficiencia energética con menor coste.
  \item Simplicidad del escalado mediante la adición de nuevos nodos a base de hardware básico y común.
\end{itemize}

En el siguiente esquema simplificado se pueden apreciar las relaciones de orden entre los distintos componentes:\\

\begin{figure}[h]
  \centering
    \includegraphics[scale=1]{plataforma}
      \caption{Plataforma de explotación}
\end{figure}

\textbf{¿Cómo?} El grueso del trabajo no se centra en la explotación de la plataforma sino en la automatización de su despliegue. Se detallará el proceso completo, desde el servidor vacío que nos entrega el fabricante hasta la primera ejecución de un trabajo en el \emph{framework} de \emph{Marathon}.\\

Para realizar la automatización se ha creado una plataforma paralela de arranque, o \emph{bootstrapping}, compuesta por seis elementos funcionales que han sido encapsulados como microservicios dentro de contenedores de \emph{Docker}. El siguiente esquema simplificado conecta con el anterior esquema de la plataforma de explotación:

\begin{figure}[h]
  \centering
    \includegraphics[scale=1]{boot_platform}
      \caption{Plataforma de arranque}
\end{figure}

Más adelante se detallará el rol de cada uno de los seis contenedores, su proceso de construcción y cómo interactúan entre ellos para aprovisionar los servidores y desplegar los servicios de la plataforma de explotación.

En el contexto de este trabajo entendemos por \emph{'aprovisionamiento de un sistema'} el conjunto de acciones que se deben realizar para disponer de un equipo, ya sea físico o virtual, con un mínimo sistema operativo instanciado y accesible a nivel TCP/IP\@. No se trata de dar mucha inteligencia al sistema de aprovisionamiento, sino de consensuar un modelo sencillo y predecible que resulte eficaz en su funcionamiento y simple de mantener.

Una vez aprovisionados, los sistemas sirven de base sobre la que podemos desplegar servicios. Los servicios determinan el rol o la razón de ser de cada equipo. Si bien en la fase de aprovisionado este rol se perfila sutilmente, no es hasta la fase de despliegue donde se definen con exactitud las funciones de cada equipo y las interrelaciones entre los mismos. Por ejemplo, usaremos herramientas como \emph{r10k} y \emph{puppet} para transformar los equipos físicos en hipervisores \emph{KVM}.

Empezaremos viendo algunos detalles sobre la plataforma de explotación y seguiremos con el funcionamiento de la plataforma de \emph{bootstrapping}.
\end{intro}

\pagestyle{fancy}

%----------------------------
% Plataforma de explotación:
%----------------------------

\chapter{Plataforma de explotación}
En este capítulo se detalla la plataforma de explotación. Veremos el \emph{hardware} que se ha utilizado para montar la maqueta, también veremos detalles sobre la disposición de la red y finalmente los distintos componentes \emph{software} que conforman la plataforma. El diseño de la plataforma ha sido concebido para dar respuesta a las dos problemáticas de escalabilidad descritas en la introducción.

\section{Hardware}
Para el proyecto se han utilizado 2 equipos hipervisores de iguales características. Cada equipo simula un rack que contiene 4 servidores. Algunos de los \emph{frameworks} que pueden ser ejecutados en la plataforma tienen consciencia de rack (\emph{rack awareness}) y distribuyen réplicas de los datos de manera que optimizan el uso de la red (\emph{data locality}) y minimizan el riesgo de pérdida.\\

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.32]{hardware}
      \caption{Hardware}
\end{figure}

La elección del hardware responde a la necesidad de reducir el consumo eléctrico y el ruido. No hay partes móviles y toda la electrónica es de estado sólido. La propia carcasa se utiliza de disipador para la \emph{CPU}. Los equipos tienen las siguientes características:

\begin{table}[h]
  \centering
  \begin{tabular}{l|c|c|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Componente}}                       & \textbf{Cantidad} & \textbf{Precio unid.}  \\ \hline
    \multicolumn{1}{|l|}{Placa base Intel DQ77KB Thin Mini-ITX}     & 2                 & 106,23 \euro           \\ \hline
    \multicolumn{1}{|l|}{Procesador Intel Xeon E3-1265Lv2 QuadCore} & 2                 & 344,93 \euro           \\ \hline
    \multicolumn{1}{|l|}{16GB (8GB x2) Crucial DDR3 1600MHz}        & 2                 & 141,30 \euro           \\ \hline
    \multicolumn{1}{|l|}{128GB SSD Crucial mSATA}                   & 2                 & 78,28 \euro            \\ \hline
    \multicolumn{1}{|l|}{Carcasa Thin Mini-ITX Akasa Euler}         & 2                 & 105,80 \euro           \\ \hline
    \multicolumn{1}{|l|}{Cisco SLM2008T-EU 8 x 10/100/1000}         & 1                 & 114,75 \euro           \\ \hline
                                                                    & \textbf{Total}    & \textbf{1667,83 \euro} \\ \cline{2-3}
  \end{tabular}
  \caption{Coste del material}
\end{table}

Cada equipo dispone de dos tarjetas de red. En el siguiente esquema se puede apreciar el conexionado físico (izquierda) y lógico (derecha). La ONT es un terminal óptico de red que conecta con la red de fibra del operador Movistar, sobre la VLAN 6 se establece una conexión PPPoE para datos y la VLAN 3 se reserva para voz. Los detalles de conexión con el operador no tienen mayor interés para este trabajo.

\begin{figure}[h]
  \centering
    \includegraphics[scale=1]{layout}
      \caption{Disposición de la red}
\end{figure}

El portátil que aparece en el esquema va a ser el equipo que contenga la plataforma de \emph{bootstrapping} que veremos en el segundo capítulo.

\clearpage

\section{Network}

La red se divide en externa e interna. La red externa es la que nos proporciona nuestro operador y normalmente nuestra capacidad para influir sobre su diseño es limitada o nula, por lo que tenemos que adaptarnos a ella. Para realizar este trabajo se ha utilizado una red doméstica FTTH de Movistar.

La red interna es la red sobre la que desplegaremos tanto la plataforma de explotación como la plataforma de \emph{bootstrapping}. Una red interna es suficiente para la simulación de este proyecto, pero en casos con carga real hay que separar las redes en función de su carga y requisitos de seguridad. Por ejemplo, podríamos dedicar una red exclusivamente a tecnologías de almacenamiento de datos.

En la siguiente figura se pueden apreciar los distintos elementos de red que conectan el mundo virtual con el mundo físico.

\begin{figure}[h]
  \centering
    \includegraphics[scale=1]{sdnet}
      \caption{Red definida mediante software}
\end{figure}

La red se define mediante software y a lo largo de distintas fases coordinadas por la plataforma de \emph{bootstrapping}. En el esquema se han coloreado de forma distinta los elementos de red en función de la pieza de software que los configura.

\section{Software}

La pila o \emph{stack} de software es aparentemente compleja, sin embargo, se repite para cada nodo del cluster de manera que el escalado horizontal se convierte en una tarea sencilla. En la siguiente figura se pueden apreciar con algo más de detalle los distintos componentes que constituyen la plataforma de explotación.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.75]{detailed_stack}
      \caption{Pila de software}
\end{figure}

Para conseguir alta disponiblilidad (HA) en un cluster real serían necesarias 3 o 5 instancias de \emph{Mesos Master} y 3 o 5 instancias de \emph{Zookeeper}. En este caso, para la prueba de concepto en esta maqueta es suficiente con una única instancia de cada uno. \emph{Zookeeper} y \emph{Etcd} son componentes que ofrecen funcionalidades muy similares y a la larga \emph{Zookeeper} será reemplazado.

A continuación, veremos algunos detalles sobre cada uno de los componentes más importantes.

\subsection{KVM}

KVM (Kernel-based Virtual Machine) es una respuesta a la necesidad de implementar virtualización completa con Linux. Se compone de un módulo del núcleo (\emph{kvm.ko}) y herramientas en el espacio de usuario. El componente KVM para el núcleo está incluido en Linux desde la versión 2.6.20.

KVM permite ejecutar máquinas virtuales utilizando imágenes de disco que contienen sistemas operativos. Cada máquina virtual tiene su propio hardware virtualizado: tarjetas de red, discos duros, CPUs, etc.

Una de las características que KVM posee es la estrategia de \emph{overcommit}. En este proyecto ésta se aplica en dos recursos: CPU y memoria. En ambos casos consiste en comprometerse a abastecer con más recursos de los que físicamente dispone el host.

\subsection{CoreUp}

\emph{CoreUp} es un script de unas 300 líneas escrito en \emph{Bash}. Su cometido principal es preparar el entorno para instanciar \emph{CoreOS}, es la `argamasa' entre KVM y \emph{CoreOS}. Una vez ejecutado no queda residente en memoria. Se encarga de:

\begin{itemize}
  \item Verificar que el entorno es el adecuado para instanciar los servidores virtuales.
  \item Descargar la imagen de disco de CoreOS\@. Esta imagen se sirve desde el contenedor \emph{data01} para evitar la dependencia con internet.
  \item Incrementar el tamaño de disco de la imagen descargada a 20GB ya que el tamaño por defecto es únicamente de 3GB.
  \item Generar de forma dinámica la configuración de \emph{cloud-config} para las distintas instancias de CoreOS.
  \item Generar dispositivos TAP de red (capa 2) para interconectar las VMs con el host anfitrión.
  \item Generar direcciones MAC deterministas (siempre las mismas) para evitar colisiones entre las máquinas virtuales.
  \item Determinar los recursos de cada VM en función de los recursos de cada hipervisor y del factor de \emph{overcommit} definido.
  \item Instanciar 4 servidores virtuales en cada hipervisor.
\end{itemize}

A continuación vemos como \emph{CoreUp} inicia por primera vez la máquina virtual \emph{core01}.
\\

\begin{lstlisting}[style=dnsmasq]
[root@kvm01 ~]# coreup core01
Checking VM is not running...                                                           [  OK  ]
Checking hostname is not in use...                                                      [  OK  ]
Checking VM limit is not reached...                                                     [  OK  ]
Downloading CoreOS image file...                                                        [  OK  ]
Resizing CoreOS disk image...                                                           [  OK  ]
Generating cloud-config...                                                              [  OK  ]
Generating TAP interfaces...                                                            [  OK  ]
Starting the virtual machine...                                                         [  OK  ]
Testing SSH port...                                                                     [  OK  ]
Restarting after first boot...                                                          [  OK  ]
Checking VM is not running...                                                           [  OK  ]
Checking hostname is not in use...                                                      [  OK  ]
Checking VM limit is not reached...                                                     [  OK  ]
Generating cloud-config...                                                              [  OK  ]
Generating TAP interfaces...                                                            [  OK  ]
Starting the virtual machine...                                                         [  OK  ]
Testing SSH port...                                                                     [  OK  ]
\end{lstlisting}

\subsection{CoreOS}

\emph{CoreOS} es una distribución de Linux minimalista. Los servicios que proporciona están orientados a cimentar infraestructuras de clusters. La distribución promueve el uso de contenedores como modelo de distribución de software abandonando así los tradicionales gestores de paquetes. Los servicios de \emph{CoreOS} que usaremos para este proyecto son \emph{Etcd}, \emph{Fleet} y \emph{Docker}.

\subsubsection{Etcd}

\emph{Etcd} es una base de datos de clave valor que utiliza el algoritmo de consenso \emph{Raft} para realizar la elección automática de su master. Los datos almacenados en \emph{Etcd} se distribuyen y replican de forma automática, de manera que los cambios se reflejan en todo el cluster.

Por sus características, se utiliza en el campo de la localización de servicios y configuraciones compartidas. Sus bajos tiempos de convergencia (todos los servidores acuerdan el valor de las claves) la hacen ideal en entornos donde se requiere tolerancia a fallos.

En la maqueta podemos consultar cuál de los nodos ha sido elegido líder del cluster ejecutando el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
core@core01# curl -L http://127.0.0.1:4001/v2/leader
http://core03:7001
\end{lstlisting}

También podemos consultar la lista de nodos que son miembros del cluster de \emph{Etcd} ejecutando el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
core@core01# curl -sL http://127.0.0.1:4001/v2/machines | sed s/,\ /\\n/g
http://core03:4001
http://core04:4001
http://core05:4001
http://core06:4001
http://core07:4001
http://core08:4001
http://core01:4001
http://core02:4001
\end{lstlisting}

\subsubsection{Fleet}

\emph{Fleet} es una herramienta de gestión de clusters que utiliza \emph{Etcd} para la compartición de configuraciones. \emph{Fleet} controla los sistemas de inicialización de servicios (\emph{systemd}) de todos los nodos del cluster, proporcionando así una gestión unificada.

Podemos consultar el listado de nodos que forman parte del cluster de \emph{Fleet} con el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
core@core01# fleetctl list-machines
MACHINE   IP    METADATA
2382b63a... 192.168.1.140 host=core06
2d5dc467... 192.168.1.62  host=core08
35b394af... 192.168.1.98  host=core05
35e7ee19... 192.168.1.119 host=core04
747ce9e7... 192.168.1.89  host=core03
99a44c29... 192.168.1.143 host=core01
a6668efd... 192.168.1.54  host=core02
f469c6df... 192.168.1.141 host=core07
\end{lstlisting}

También podemos consultar el estado de los servicios controlados por \emph{Fleet} utilizando el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
core@core01# fleetctl list-units
UNIT      MACHINE       ACTIVE  SUB
marathon.service     99a44c29.../192.168.1.143 active  running
mesos-master.service 99a44c29.../192.168.1.143 active  running
mesos-slave.service  2382b63a.../192.168.1.140 active  running
mesos-slave.service  2d5dc467.../192.168.1.62  active  running
mesos-slave.service  35b394af.../192.168.1.98  active  running
mesos-slave.service  35e7ee19.../192.168.1.119 active  running
mesos-slave.service  747ce9e7.../192.168.1.89  active  running
mesos-slave.service  99a44c29.../192.168.1.143 active  running
mesos-slave.service  a6668efd.../192.168.1.54  active  running
mesos-slave.service  f469c6df.../192.168.1.141 active  running
zookeeper.service    a6668efd.../192.168.1.54  active  running
\end{lstlisting}

En la plataforma de explotación se utiliza \emph{Fleet} para arrancar todos los servicios relacionados con \emph{Mesos} incluidos sus \emph{frameworks}, por lo tanto también \emph{Marathon}.

\subsubsection{Docker}

\emph{Docker} es un proyecto de código abierto que automatiza el despliegue de aplicaciones encapsulándolas dentro de contenedores de software. \emph{Docker} utiliza tecnologías de aislamiento de recursos, como los \emph{cgroups} o los \emph{namespaces}, que se encuentran disponibles en el kernel de Linux.

En una misma instancia de Linux se pueden crear múltiples contenedores, independientes los unos de los otros y sin la sobrecarga que supondría crear máquinas virtuales.

La visión del sistema operativo que tiene la aplicación queda completamente aislada, incluyendo el árbol de procesos, la pila de red, los sistemas de ficheros y los identificadores de usuarios, así como los recursos de CPU, memoria, I/O y red.

Las principales ventajas que aporta la containerización son:
\begin{itemize}
  \item \textbf{Inmutabilidad:} Entornos estáticos para albergar las aplicaciones y proporcionar fiabilidad en el despliegue de las mismas cuando éste se realiza en plataformas distintas a la de desarollo.
  \item \textbf{Portabilidad:} Los contenedores son artefactos ejecutables que se pueden mover, almacenar, copiar o destruir con facilidad.
  \item \textbf{Desacoplamiento:} Las aplicaciones más complejas se componen a base de múltiples aplicaciones ligeras que se interrelacionan entre sí (microservicios).
\end{itemize}

En cualquier instancia de \emph{CoreOS} de la maqueta se puede consultar el listado de imágenes de contenedores descargadas y listas para ser instanciadas:\\

\begin{lstlisting}[style=dnsmasq]
core@core01# docker images
REPOSITORY                          TAG          IMAGE ID           CREATED           VIRTUAL SIZE
regi01.demo.lan:5000/marathon       latest       beccb9c8de89       5 weeks ago       1.248 GB
regi01.demo.lan:5000/marathon       v0.7.5       beccb9c8de89       5 weeks ago       1.248 GB
regi01.demo.lan:5000/mesos-slave    0.20.1       cb1584229bc1       4 months ago      1.168 GB
regi01.demo.lan:5000/mesos-slave    latest       cb1584229bc1       4 months ago      1.168 GB
regi01.demo.lan:5000/mesos-master   0.20.1       fd7f7e5c4513       4 months ago      1.168 GB
regi01.demo.lan:5000/mesos-master   latest       fd7f7e5c4513       4 months ago      1.168 GB
\end{lstlisting}

También podemos consultar el listado de contenedores instanciados:\\

\begin{lstlisting}[style=dnsmasq]
core@core01# docker ps
CONTAINER ID  IMAGE                COMMAND              CREATED        STATUS       NAMES
b709ae465b18  mesos-master:0.20.1  mesos-master --ip=1  10 hours ago   Up 10 hours  mesos_master
f1cfa94270cb  marathon:latest      ./bin/start --maste  10 hours ago   Up 10 hours  marathon
e18163eab926  mesos-slave:0.20.1   mesos-slave --ip=19  10 hours ago   Up 10 hours  mesos_slave
\end{lstlisting}

\subsection{Mesos}

\emph{Mesos} proporciona una capa de abstracción sobre los recursos de CPU, de memoria y de disco presentes en flotas de servidores. Esta capa ligera sirve de base sobre la que construir sistemas distribuidos, elásticos y tolerantes a fallos.

De forma análoga a la relación que se establece entre las aplicaciones, el kernel y los recursos de hardware, \emph{Mesos} sirve de gestor de recursos y planificador de tareas entre los \emph{frameworks} y los recursos hardware del centro de datos. En otras palabras, \emph{Mesos} es el kernel del centro de datos.

\emph{Mesos} ofrece recursos a los \emph{frameworks} y éstos pueden aceptar o declinar las ofertas. En caso de ser aceptadas, es tarea de los \emph{frameworks} planificar los trabajos que se van a llevar a cabo con los recursos adquiridos. Las ofertas son bloqueantes lo que significa que no se ofrecerán los mismos recursos a dos \emph{frameworks} distintos simultáneamente. Por todo ello, también se describe a \emph{Mesos} como un `planificador pesimista de dos niveles'.

En la maqueta del proyecto podemos consultar el listado de tareas en ejecución utilizando un contenedor creado específicamente para ejecutar comandos en \emph{Mesos}:\\

\begin{lstlisting}[style=dnsmasq]
# docker run -it --rm --env MESOS_MASTER=zk://core02:2181/mesos h0tbird/mcli mesos ps

   TIME   STATE   RSS    CPU  %MEM          COMMAND          USER                        ID
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
 0:00:00    R    0.00 B   0   0.00  sh -c 'nc -p $PORT0...'  root  hostname.11e4-8c27-0242ac110002
\end{lstlisting}

\subsubsection{Marathon}

\emph{Marathon} es un \emph{framework} de \emph{Mesos} especialmente diseñado para ejecutar aplicaciones que tienen una esperanza de vida larga como, por ejemplo, un servidor web \emph{Apache} o una base de datos de clave valor \emph{Redis}.

\emph{Marathon} y \emph{Fleet} son herramientas similares. Las dos son consideradas \emph{`init systems'} distribuidos; sin embargo, \emph{Marathon} ofrece más prestaciones que \emph{Fleet}. Es por ello que el papel de \emph{Fleet} queda relegado a arrancar los servicios de \emph{Mesos} y \emph{Marathon}.

\emph{Marathon} proporciona una API REST para arrancar, parar y escalar applicaciones, está escrito en \emph{Scala} y puede funcionar en alta disponibilidad arrancando múltiples instancias.

El siguiente ejemplo muestra cómo arrancar una aplicación muy simple en \emph{Marathon}.\\

\begin{lstlisting}[style=dnsmasq]
%curl -s -H "Content-Type: application/json" http://core01:8080/v2/apps -d \
%  ' {
%      "id": "hostname",
%      "cmd": "nc -p $PORT0 -ll -e hostname",
%      "cpus": 0.2,
%      "mem": 2.0,
%      "instances": 1,
%      "container": {
%        "type": "DOCKER",
%        "docker": {
%          "image": "busybox:latest"
%        }
%      }
%    }
%  ' | jq '.'
\end{lstlisting}

%-------------------------
% Plataforma de arranque:
%-------------------------

\chapter{Plataforma de arranque}

\section{A vista de pájaro}

La plataforma de arranque o \emph{bootstrapping} está compuesta por seis contenedores de \emph{Docker} conectados entre sí mediante un \emph{bridge} que, a su vez, los conecta con la red física. En el siguiente esquema se representan los seis contenedores, las interfaces de red y los sistemas físicos que queremos aprovisionar y que constituirán la plataforma de explotación:

\begin{wrapfigure}{r}{0.35\textwidth}
\vspace{-15pt}
\begin{center}
  \includegraphics[scale=1]{booddies2}
  \caption{Plataforma de arranque}
\end{center}
\vspace{-110pt}
\end{wrapfigure}

En la tabla de abajo se detallan, para cada uno de los seis contenedores, los siguientes conceptos:

\begin{itemize}[leftmargin=20pt]
  \item \textbf{Imagen:} Nombre público de la imagen de \emph{Docker} que se utiliza como base sobre la que instanciar el contenedor.
  \item \textbf{Container ID:} Identificador del contenedor una vez instanciado a partir de su imagen base.
  \item \textbf{Binario:} Ruta completa al binario que se ejecuta como único proceso dentro del contenedor.
  \item \textbf{Servicios:} Listado de los servicios proporcionados por cada instancia del contenedor.
\end{itemize}

\vspace{30pt}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|l|c|}
\hline
\multicolumn{1}{|c|}{\textbf{Imagen}}    & \textbf{Container ID} & \multicolumn{1}{c|}{\textbf{Binario}} & \textbf{Servicio} \\ \hline
h0tbird/boot                             & boot01                & \textit{/usr/sbin/dnsmasq}            & PXE,DHCP,TFTP,DNS \\ \hline
h0tbird/data                             & data01                & \textit{/usr/sbin/httpd}              & HTTP,YUM          \\ \hline
h0tbird/gito                             & gito01                & \textit{/usr/sbin/sshd}               & SSH,GIT           \\ \hline
h0tbird/cgit                             & cgit01                & \textit{/usr/sbin/httpd}              & HTTP,CGIT         \\ \hline
h0tbird/regi                             & regi01                & \textit{/usr/bin/gunicorn}            & Docker registry   \\ \hline
h0tbird/ntpd                             & ntpd01                & \textit{ntpd}                         & NTP               \\ \hline
\end{tabular}
\caption{Imágenes, contenedores y servicios}
\end{table}

Más adelante, en el capítulo 4, veremos cómo instalar la plataforma de arranque. En el presente capítulo se supone que la plataforma ya ha sido instalada de manera que podemos explorar su estado utilizando varios comandos que nos darán visión sobre los contenedores y los distintos elementos de red.

\section{Explorando la plataforma}

El comando \emph{'docker images'} sirve para mostrar el listado de imágenes de \emph{Docker} que han sido previamente descargadas en el sistema local:\\

\begin{lstlisting}[style=dnsmasq]
# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
h0tbird/gito        latest              c48d28d1b611        8 days ago          321.7 MB
h0tbird/cgit        latest              8d41334e9359        8 days ago          338.6 MB
h0tbird/regi        latest              f5cd3db745e7        8 days ago          286.4 MB
h0tbird/data        latest              77d8e6f858f7        8 days ago          317.8 MB
h0tbird/boot        latest              c58be2afc754        8 days ago          250.7 MB
h0tbird/ntpd        latest              1234567890ab        8 days ago          233.7 MB
\end{lstlisting}

El comando \emph{'docker ps'} sirve para mostrar el listado de contenedores instanciados a partir de las imágenes anteriores:\\

\begin{lstlisting}[style=dnsmasq]
# docker ps
CONTAINER ID    IMAGE                  COMMAND                 CREATED        STATUS        NAMES
1c804df5ba93    h0tbird/cgit:latest    "/init /usr/sbin/htt    7 hours ago    Up 7 hours    cgit01
196677cc770b    h0tbird/data:latest    "/init /usr/sbin/htt    7 hours ago    Up 7 hours    data01
4438c7893f3f    h0tbird/regi:latest    "/init /usr/bin/pyth    7 hours ago    Up 7 hours    regi01
c1c76bdde097    h0tbird/gito:latest    "/init /usr/sbin/ssh    7 hours ago    Up 7 hours    gito01
b989bf902735    h0tbird/boot:latest    "/init /usr/sbin/dns    7 hours ago    Up 7 hours    boot01
1234567890ab    h0tbird/ntpd:latest    "/init /something/so    7 hours ago    Up 7 hours    ntpd01
\end{lstlisting}

El comando \emph{'brctl show'} sirve para mostrar el estado de los \emph{'bridges'} así como el listado de las \emph{'interfaces'} que tienen vinculadas:\\

\begin{lstlisting}[style=dnsmasq]
# brctl show
bridge name    bridge id          STP   enabled interfaces
br0            8000.5215caf87b70  no    eth0
                                        veth-boot01
                                        veth-data01
                                        veth-gito01
                                        veth-ntpd01
                                        veth-regi01
\end{lstlisting}

Podemos, también, ejecutar \emph{'systemctl status'} para consultar el estado de los distintos servicios:\\

\begin{lstlisting}[style=dnsmasq]
# systemctl status boot data gito cgit regi ntpd
\end{lstlisting}

A continuación veremos detalles sobre cada uno de los seis contenedores usados en la plataforma de arranque.

\clearpage

\section{Los seis contenedores}

\subsection{boot01}

El contenedor \emph{boot01} es el primer contenedor de la plataforma de \emph{bootstrapping} que hay que arrancar puesto que se encargará de asignar direcciones \emph{IP} a los cinco contenedores restantes.
Podemos inspeccionar su estado con el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
# journalctl -f -u boot
\end{lstlisting}

Dentro de \emph{boot01} se ejecuta \emph{'dnsmasq'} como único proceso. \emph{Dnsmasq} proporciona infraestructura de red aportando funcionalidades tales como DHCP, PXE, TFTP y DNS.

A parte de la propia imagen del contenedor (descargada de forma automática la primera vez que se inicia el servicio), \emph{boot01} dispone de los siguientes ficheros de configuración y directorios de datos:

\begin{table}[h]

  \centering

  \begin{tabular}{ll}
    \toprule
    \textbf{Configuración}        & \textit{/etc/booddies/boot.conf}          \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Fichero de servicio}  & \textit{/etc/systemd/system/boot.service} \\
    \midrule
    \textbf{Fichero de control}   & \textit{/usr/local/sbin/runctl-boot}      \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Directorio de datos}  & \textit{/data/boot}                       \\
    \bottomrule
  \end{tabular}

  \caption{Ficheros y directorios de boot01}

\end{table}

La imagen del contenedor ha sido producida aplicando la siguiente receta o \emph{Dockerfile}:\\

\begin{lstlisting}[style=dnsmasq]
FROM centos:7
MAINTAINER Marc Villacorta Morera <marc.villacorta@gmail.com>
RUN rpm --import http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
RUN yum update -y && yum clean all
RUN yum install -y dnsmasq syslinux-tftpboot && yum clean all
RUN mkdir /tftpboot && \
ln /var/lib/tftpboot/pxelinux.0 /tftpboot/ && \
ln /var/lib/tftpboot/menu.c32 /tftpboot/
ADD rootfs /
ENTRYPOINT ["/init", "/usr/sbin/dnsmasq", "-k"]
\end{lstlisting}

\subsection{data01}

El contenedor \emph{data01} arranca en paralelo con los contenedores \emph{gito01}, \emph{regi01} y \emph{ntpd01}.

Este contenedor ejecuta un servidor web \emph{Apache} que se utiliza como servidor de ficheros estáticos, principalmente sirve repositorios \emph{YUM}, la imagen del instalador \emph{Anaconda}, la imagen de disco del sistema operativo \emph{CoreOS} y ficheros \emph{kickstart}.

Podemos ver la actividad del servidor web con el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
# docker exec -it data01 bash -c "tail -f /var/log/httpd/*"
\end{lstlisting}

A continuación se presenta la relación de directorios de datos y ficheros de configuración que influyen sobre el comportamiento de \emph{data01}:

\begin{table}[h]

  \centering

  \begin{tabular}{ll}
    \toprule
    \textbf{Configuración}        & \textit{/etc/booddies/data.conf}          \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Fichero de servicio}  & \textit{/etc/systemd/system/data.service} \\
    \midrule
    \textbf{Fichero de control}   & \textit{/usr/local/sbin/runctl-data}      \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Directorio de datos}  & \textit{/data/data}                       \\
    \bottomrule
  \end{tabular}

  \caption{Ficheros y directorios de data01}

\end{table}

La imagen del contenedor ha sido producida aplicando la siguiente receta o \emph{Dockerfile}:\\

\begin{lstlisting}[style=dnsmasq]
FROM centos:7
MAINTAINER Marc Villacorta Morera <marc.villacorta@gmail.com>
RUN rpm --import http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
RUN rpm --import http://yum.puppetlabs.com/RPM-GPG-KEY-puppetlabs
RUN rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7
RUN yum update -y && yum clean all
RUN yum install -y epel-release \
    http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm && \
    yum clean all
RUN yum install -y httpd wget rsync yum-utils createrepo repoview && \
    yum clean all
RUN rm -f /etc/httpd/conf.d/welcome.conf
ADD rootfs /
ENTRYPOINT ["/init", "/usr/sbin/httpd", "-DFOREGROUND"]
\end{lstlisting}

\subsection{gito01}

El contenedor \emph{gito01} ejecuta un servidor \emph{SSH} que tiene acoplado un backend de \emph{Gitolite}. De esta forma se utiliza \emph{SSH} como protocolo de transporte y \emph{Gitolite} como gestor de acceso a los repositorios \emph{Git}.

Los ficheros de configuración y directorios de datos asociados a \emph{gito01} son los siguientes:
\begin{table}[h]

  \centering

  \begin{tabular}{ll}
    \toprule
    \textbf{Configuración}        & \textit{/etc/booddies/gito.conf}          \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Fichero de servicio}  & \textit{/etc/systemd/system/gito.service} \\
    \midrule
    \textbf{Fichero de control}   & \textit{/usr/local/sbin/runctl-gito}      \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Directorio de datos}  & \textit{/data/gito}                       \\
    \bottomrule
  \end{tabular}

  \caption{Ficheros y directorios de gito01}

\end{table}

La imagen del contenedor ha sido producida aplicando la siguiente receta o \emph{Dockerfile}:\\

\begin{lstlisting}[style=dnsmasq]
FROM centos:7
MAINTAINER Marc Villacorta Morera <marc.villacorta@gmail.com>
RUN rpm --import http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
RUN rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7
RUN yum update -y && yum clean all
RUN yum install -y epel-release && yum clean all
RUN yum install -y gitolite3 hostname openssh-server && yum clean all
RUN adduser -m -G gitolite3 git && \
    mkdir /home/git/.ssh && \
    chmod 700 /home/git/.ssh
ADD rootfs /
VOLUME /home/git/projects
VOLUME /home/git/repositories
ENTRYPOINT ["/init", "/usr/sbin/sshd", "-D"]
\end{lstlisting}

\subsection{cgit01}

El contenedor \emph{cgit01} utiliza la misma pila TCP/IP que el contenedor \emph{gito01} y por ello tiene que arrancar después de que \emph{gito01} haya inicializado su configuración de red.

El hecho de compartir la pila TCP/IP permite que los dos contenedores se comuniquen entre sí utilizando la dirección local \emph{127.0.0.1}. Además, los dos contenedores también comparten un par de volúmenes de datos de manera que los dos tienen visibilidad sobre los datos escritos en los volúmenes compartidos.

Podemos ver la actividad de \emph{cgit} ejecutando el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
# docker exec -it cgit01 bash -c "tail -f /var/log/httpd/*"
\end{lstlisting}

Los ficheros de configuración y directorios de datos asociados a \emph{cgit01} son los siguientes:

\begin{table}[h]

  \centering

  \begin{tabular}{ll}
    \toprule
    \textbf{Configuración}        & \textit{/etc/booddies/cgit.conf}          \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Fichero de servicio}  & \textit{/etc/systemd/system/cgit.service} \\
    \midrule
    \textbf{Fichero de control}   & \textit{/usr/local/sbin/runctl-cgit}      \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Directorio de datos}  & \textit{/data/cgit}                       \\
    \bottomrule
  \end{tabular}

  \caption{Ficheros y directorios de cgit01}

\end{table}

La imagen del contenedor ha sido producida aplicando la siguiente receta o \emph{Dockerfile}:\\

\begin{lstlisting}[style=dnsmasq]
FROM centos:7
MAINTAINER Marc Villacorta Morera <marc.villacorta@gmail.com>
RUN rpm --import http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
RUN rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7
RUN yum update -y && yum clean all
RUN yum install -y epel-release && yum clean all
RUN yum install -y cgit highlight policycoreutils-python httpd && yum clean all
RUN adduser -m git && chmod 755 /home/git && \
    rm -f /etc/httpd/conf.d/welcome.conf
ADD rootfs /
ENTRYPOINT ["/init", "/usr/sbin/httpd", "-DFOREGROUND"]
\end{lstlisting}

\subsection{regi01}

El contenedor \emph{regi01} contiene un \emph{'Docker Registry'} que sirve para almacenar y posteriormente recuperar imágenes de contenedores de \emph{Docker}. De forma análoga a la relación que se establece entre los paquetes \emph{RPM} y los repositorios \emph{YUM}, podemos considerar a \emph{'Docker Registry'} como un repositorio de imágenes de contenedores de \emph{Docker}.

Podemos ver la actividad del contenedor \emph{regi01} con el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
# journalctl -f -u regi
\end{lstlisting}

Los ficheros de configuración y directorios de datos asociados a \emph{regi01} son los siguientes:

\begin{table}[h]

  \centering

  \begin{tabular}{ll}
    \toprule
    \textbf{Configuración}        & \textit{/etc/booddies/regi.conf}          \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Fichero de servicio}  & \textit{/etc/systemd/system/regi.service} \\
    \midrule
    \textbf{Fichero de control}   & \textit{/usr/local/sbin/runctl-regi}      \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Directorio de datos}  & \textit{/data/regi}                       \\
    \bottomrule
  \end{tabular}

  \caption{Ficheros y directorios de regi01}

\end{table}

La imagen del contenedor ha sido producida aplicando la siguiente receta o \emph{Dockerfile}:\\

\begin{lstlisting}[style=dnsmasq]
FROM centos:7
MAINTAINER Marc Villacorta Morera <marc.villacorta@gmail.com>
RUN rpm --import http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
RUN rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7
RUN yum update -y && yum clean all
RUN yum install -y epel-release && yum clean all
RUN yum install -y docker-registry && yum clean all
ADD rootfs /
ENTRYPOINT [``/init'', ``/usr/bin/python'', ``/usr/bin/gunicorn'', ``--access-logfile'',
``-'', ``--error-logfile'', ``-'', ``--debug'', ``--max-requests'', ``100'',
``--graceful-timeout'',``3600'', ``-t'', ``3600'', ``-k'', ``gevent'', ``-b'', ``0.0.0.0:5000'',
``-w'', ``8'', ``docker_registry.wsgi:application'']
\end{lstlisting}

\subsection{ntpd01}

El contenedor \emph{ntpd01} proporciona el servicio de sincronización de relojes. En sistemas distribuidos es muy importante disponer de una base de tiempo fiable y común.

Para la maqueta de este proyecto no es estrictamente necesario utilizar este contenedor puesto que la sincronización se puede realizar contra servidores externos en internet.

En un entorno productivo y autocontenido sí que toma mucha importancia el hecho de disponer de una fuente de tiempo fiable y altamente disponible. Se recomienda conectar el servidor NTP a la red GPS (\emph{stratum-0}) y conseguir de esta manera estar a un único salto de una fuente de referencia (\emph{stratum-1}).

Podemos ver la actividad del contenedor \emph{ntpd01} con el siguiente comando:\\

\begin{lstlisting}[style=dnsmasq]
# journalctl -f -u ntpd
\end{lstlisting}

Los ficheros de configuración y directorios de datos asociados a \emph{ntpd01} son los siguientes:

\begin{table}[h]

  \centering

  \begin{tabular}{ll}
    \toprule
    \textbf{Configuración}        & \textit{/etc/booddies/ntpd.conf}          \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Fichero de servicio}  & \textit{/etc/systemd/system/ntpd.service} \\
    \midrule
    \textbf{Fichero de control}   & \textit{/usr/local/sbin/runctl-ntpd}      \\
    \midrule
    \rowcolor[HTML]{EFEFEF}
    \textbf{Directorio de datos}  & \textit{/data/ntpd}                       \\
    \bottomrule
  \end{tabular}

  \caption{Ficheros y directorios de ntpd01}

\end{table}

%----------------
% Bootstrapping:
%----------------

\chapter{Secuencia de arranque}

\section{Pre arranque}
Al pulsar el botón de encendido en un servidor, se inicia el proceso de pre arranque o \emph{`pre boot sequence'}. En el mismo instante en que existe diferencia de potencial, se empiezan a ejecutar una serie de procedimientos basados en hardware que realizan comprobaciones funcionales de la propia electrónica del equipo.

Aquellos componentes electrónicos que dispongan de capacidades \emph{POST} (\emph{Power-On Self Test}), ejecutarán sus rutinas como parte de la secuencia de pre arranque. Si se produjera un error en esta fase, se cancelaría la siguiente fase de arranque y se notificaría el hecho mediante señales acústicas, leds o displays, si el equipo dispone de ellos.

\begin{figure}[h]
  \centering
    \includegraphics[scale=1]{pre_arranque}
      \caption{Pre arranque}
\end{figure}

\section{Arranque}
Durante el arranque o \emph{`bootstrapping'} se pasa por una serie de fases encadenadas. Cada fase se encarga de preparar el entorno para cargar y ejecutar la siguiente de manera que, en cada fase, se aumentan las funcionalidades y la complejidad del código ejecutado.

La lógica y los datos de las funcionalidades más básicas constituyen el \emph{firmware} del equipo. Éste se encuentra a salvo en direcciones fijas dentro de chips de memoria persistente (\emph{ROM}, \emph{EPROM}, \emph{Flash}) y suele limitarse a proporcionar servicios a una capa superior de software. La vieja y conocida \emph{BIOS} (\emph{Basic Input/Output System}) o la nueva \emph{UEFI} (\emph{Unified Extensible Firmware Interface}), son buenos ejemplos de ello.

Una de las funcionalidades del \emph{firmware} que más nos interesa es la de \emph{`first-stage boot loader'} que se ocupa de localizar, recuperar, cargar en memoria y transferir el control de ejecución al software de arranque o \emph{`bootstrap program'}.

Ejemplos típicos de aplicaciones que encontramos en la categoría de software de arranque son: \emph{LILO}, \emph{GRUB}, \emph{Syslinux}, \emph{PXELinux.0} e incluso podríamos considerar al mismo \emph{kernel} de Linux en esta categoría. Los cuatro primeros se utilizan como \emph{`second-stage boot loaders'} y permiten al usuario gestionar una gran diversidad de opciones de arranque.

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.9]{arranque}
      \caption{Arranque}
\end{figure}

\emph{LILO}, \emph{GRUB} y \emph{Syslinux} suelen encontrarse ubicados en el \emph{MBR} (\emph{Master Boot Record}) del dispositivo de arranque, pudiendo ser éste un disco duro, un disquete o un \emph{DVD}. \emph{PXELinux.0} es una variante de \emph{Syslinux} que, a diferencia de los anteriores, se encuentra ubicado en un equipo remoto de la red local.

Puesto que no es lo mismo recuperar el software de arranque desde un \emph{MBR} que desde un equipo remoto, el \emph{firmware} que quiera ejecutar \emph{PXELinux.0}, va a tener que implementar una solución específica. A continuación veremos las tecnologías que lo hacen posible.

\subsection{PXE}
\emph{PXE} (\emph{Pre-Execution Environment}) extiende las funcionalidades del \emph{firmware} dotándolo de capacidades comunicativas gracias a la inclusión de los protocolos \emph{IPv4}, \emph{UDP}, \emph{DHCP} y \emph{TFTP}, estos dos últimos con leves modificaciones. Con estas extensiones, el \emph{firmware} es capaz de localizar, descargar y transferir el control de ejecución al software de arranque \emph{PXELinux.0}, también conocido con el nombre genérico de \emph{NBP} (\emph{Network Bootstrap Program}), que se encuentra ubicado en otro equipo de la red local.

De forma resumida, la mecánica del protocolo \emph{PXE} funciona de la siguiente manera. El cliente inicia el protocolo mediante un broadcast conocido como \emph{DHCPDISCOVER} que contiene unas extensiones (opciones 128 a 135 del \emph{RFC} 4578) que identifican la solicitud como procedente de un equipo que implementa el protocolo \emph{PXE}. En el log del servidor que recibe la solicitud podemos ver los siguientes detalles:\\

\begin{lstlisting}[style=dnsmasq]
DHCPDISCOVER(eth1) 00:30:1b:a0:6f:cc
requested options: 1:netmask, 2:time-offset, 3:router, 5, 6:dns-server,
requested options: 11, 12:hostname, 13:boot-file-size, 15:domain-name,
requested options: 16:swap-server, 17:root-path, 18:extension-path,
requested options: 43:vendor-encap, 54:server-identifier, 60:vendor-class,
requested options: 67:bootfile-name, (*@\textcolor{softred}{128}@*), (*@\textcolor{softred}{129}@*), (*@\textcolor{softred}{130}@*), (*@\textcolor{softred}{131}@*), (*@\textcolor{softred}{132}@*),
requested options: (*@\textcolor{softred}{133}@*), (*@\textcolor{softred}{134}@*), (*@\textcolor{softred}{135}@*)
\end{lstlisting}

Asumiendo que en la red existe un servidor \emph{DHCP} o un proxy \emph{DHCP} que también implementa las mismas extensiones del protocolo \emph{DHCP}, el servidor envía al cliente una lista de servidores de arranque (el servidor redirige al cliente) y el nombre del fichero de arranque que el cliente tiene que descargar (\emph{pxelinux.0} en nuestro caso).\\

\begin{lstlisting}[style=dnsmasq]
dnsmasq-dhcp[15]: DHCPOFFER(eth1) 192.168.1.51 00:30:1b:a0:6f:cc
dnsmasq-dhcp[15]: (*@\textcolor{softred}{next server: 192.168.1.2}@*)
dnsmasq-dhcp[15]: sent size:  1 option: 53 message-type  2
dnsmasq-dhcp[15]: option: 54 server-identifier  192.168.1.2
dnsmasq-dhcp[15]: option: 51 lease-time  12h
dnsmasq-dhcp[15]: option: 58 T1  6h
dnsmasq-dhcp[15]: option: 59 T2  10h30m
dnsmasq-dhcp[15]: option: 67 (*@\textcolor{softred}{bootfile-name  pxelinux.0}@*)
dnsmasq-dhcp[15]: option:  1 netmask  255.255.255.0
dnsmasq-dhcp[15]: option: 28 broadcast  192.168.1.255
dnsmasq-dhcp[15]: option:  3 router  192.168.1.1
dnsmasq-dhcp[15]: option:  6 dns-server  192.168.1.2
dnsmasq-dhcp[15]: available DHCP range: 192.168.1.50 -- 192.168.1.150
\end{lstlisting}

A continuación, el cliente utiliza \emph{TFTP} para descargar el \emph{NBP} de uno de los servidores de arranque. Finalmente, el \emph{firmware} del cliente transfiere el control de ejecución a la imagen descargada.\\

\begin{lstlisting}[style=dnsmasq]
dnsmasq-tftp[15]: (*@\textcolor{softred}{sent /tftpboot/pxelinux.0 to 192.168.1.51}@*)
\end{lstlisting}

\subsubsection{PXELinux.0}
\emph{PXELinux.0} actua como \emph{`Second Stage Boot Loader'} permitiendo localizar, descargar y pasar el control de ejecución al \emph{kernel} de Linux y a su correspondiente \emph{initramfs} (\emph{Initial RAM File System}). \emph{PXELinux.0} comunica al \emph{kernel} la dirección de memoria en la que ha cargado el \emph{initramfs}. La ejecución del \emph{kernel} puede ser parametrizada mediante un fichero de configuración que además, permite definir menús interactivos. El primer fichero que \emph{PXELinux.0} intenta descargar del servidor \emph{TFTP} es por tanto, su propio fichero de configuración.

Cada cliente puede disponer de su propio fichero de configuración. El nombre del fichero de cada equipo queda determinado por su dirección \emph{MAC} o la conversión a hexadecimal de su dirección \emph{IP}. Si no existe fichero alguno con ese nombre, se procede a eliminar un dígito hexadecimal del final del nombre y se repite la búsqueda sucesivamente tal y como se muestra en la siguiente captura:\\

\begin{lstlisting}[style=dnsmasq]
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/01-00-30-1b-a0-6f-cc not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0A80233 not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0A8023 not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0A802 not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0A80 not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0A8 not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0A not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C0 not found
dnsmasq-tftp[15]: file /tftpboot/pxelinux.cfg/C not found
dnsmasq-tftp[15]: (*@\textcolor{softred}{sent /tftpboot/pxelinux.cfg/default to 192.168.1.51}@*)
dnsmasq-tftp[15]: (*@\textcolor{softred}{sent /tftpboot/menu.c32 to 192.168.1.51}@*)
\end{lstlisting}

Si finalmente no se encuentra ninguna configuración, se utilizará el fichero por defecto llamado \emph{`default'}. Para la realización de este proyecto se ha definido que todos los equipos físicos dispongan de configuración específica y que las máquinas virtuales usen la configuración por defecto. Por ejemplo, al equipo físico que realizará funciones de hipervisor se le aplica la siguiente configuración:\\

\begin{lstlisting}[style=dnsmasq]
DEFAULT menu.c32
PROMPT 0
TIMEOUT 100
ONTIMEOUT local

MENU TITLE Main Menu

LABEL local
  MENU LABEL Boot local hard drive
  LOCALBOOT 0

LABEL install
  MENU LABEL Unattended CentOS 7 installation
  KERNEL images/centos/7/x86_64/vmlinuz
  IPAPPEND 2
  APPEND initrd=images/centos/7/x86_64/initrd.img rd.driver.pre=loop
         inst.repo=http://data01.demo.lan/centos/7/os/x86_64/
         inst.ks=http://data01.demo.lan:/kickstarts/akasa.ks
         inst.geoloc=0 inst.text
\end{lstlisting}

Tras descargar el fichero de configuración de \emph{PXELinux.0} se procede a su interpretación. La primera línea (\emph{DEFAULT menu.c32}) provoca la descarga mediante \emph{TFTP} de otro binario cuya única función es presentar un menú interactivo en la consola del sistema. El resto del fichero contiene la configuración de dicho menú. Cada elemento seleccionable contiene la información necesaria para localizar a una pareja de \emph{kernel} y \emph{initramfs} así como los parámetros adicionales que se le quieran entregar al \emph{kernel} en el momento de su ejecución.

El aspecto del menú que nos presenta \emph{menu.c32} es el siguiente:\\

\begin{lstlisting}[style=dnsmasq]
                   +----------------------------------------------------------+
                   |                        Main Menu                         |
                   +----------------------------------------------------------+
                   | Boot local hard drive                                    |
                   | Unattended CentOS 7 installation                         |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   |                                                          |
                   +----------------------------------------------------------+

                                     Press [Tab] to edit options

                                    Automatic boot in 7 seconds...
\end{lstlisting}

Si pulsamos \emph{[Tab]} sobre la opción \emph{`Unattended CentOS 7 installation'} podremos editar la línea del comando que se ejecuta al seleccionar esta opción:\\

\begin{lstlisting}[style=dnsmasq]
> images/centos/7/x86_64/vmlinuz initrd=images/centos/7/x86_64/initrd.img rd.driver.pre=loop inst.
repo=http://data01.demo.lan/centos/7/os/x86_64/ inst.ks=http://data01.demo.lan:/kickstarts/akasa.ks
inst.geoloc=0 inst.text BOOTIF=01-00-30-1b-a0-6f-cc
\end{lstlisting}

Los parámetros que no tienen el prefijo \emph{`inst'} serán interpretados directamente por el \emph{kernel} o por el subsistema \emph{Dracut}. Los parámetros que si que tienen el prefijo \emph{`inst'} van a ser interpretados más adelante por el instalador \emph{Anaconda}. El parámetro \emph{BOOTIF} que aparece al final, es consecuencia de la opción \emph{IPAPPEND 2} que hemos añadido en la configuración de \emph{PXELinux.0}. La dirección \emph{MAC} que aparece, corresponde con la interfície de red que ha iniciado el proceso \emph{PXE}, este valor también será usado más adelante por \emph{Anaconda}.

En el siguiente esquema se representan las interacciones que se establecen entre los gestores de arranque del servidor que queremos aprovisionar \emph{kvm01} y el contenedor de arranque \emph{boot01}:\\

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.85]{bootstrap_1}
      \caption{Gestor de arranque}
\end{figure}

\subsection{Dracut}
Con el \emph{kernel} y el \emph{initramfs} ya cargados en memoria, \emph{PXELinux.0} entrega el control de ejecución, la dirección de memoria donde se encuentra el \emph{initramfs} y los parámetros de arranque al \emph{kernel} de Linux. El \emph{kernel} se descomprime a sí mismo y extrae el \emph{initramfs} (que se encuentra en formato \emph{cpio}) montándolo en el sistema de archivos raíz o \emph{`root file system (rootfs)'} para, finalmente, transferir el control de ejecución al binario \emph{/init} que ahora se encuentra en la raíz del nuevo \emph{rootfs}.

En este contexto \emph{Dracut} tiene dos roles. Por un lado, es la herramienta de línea de comandos que se usa para crear el \emph{initramfs} y, por otro lado, es el propio subsistema que se encierra dentro del \emph{initramfs} y que es invocado al ejecutar \emph{/init}. La única tarea del subsistema \emph{Dracut} es localizar, cargar y conmutar a la imagen final de \emph{rootfs} que contiene el instalador \emph{Anaconda} y que se usará durante el posterior proceso de instalación.

\subsubsection{Herramienta de línea de comandos}
Normalmente no es necesario crear un \emph{initramfs} a medida. Sin embargo, uno de los servidores que se han utilizado para realizar pruebas necesita soporte específico para una tarjeta de red \emph{Nvidia}.
Para generar el nuevo \emph{initramfs} que incorpora el driver obsoleto \emph{forcedeth}, se ha utilizado un contenedor de \emph{Docker} tal y como se muestra a continuación:\\

\begin{lstlisting}[style=dnsmasq]
$ docker run -it --rm -e KVER='3.10.0-123' -v ${PWD}/initramfs:/initramfs h0tbird/centos bash -c "
sed -i '/exclude/d' /etc/yum.conf && \
yum clean all && \
yum -y update && \
yum install -y kernel-\${KVER}.el7 *dracut* vi tar gzip dd rpcbind nfs-utils \
http://elrepo.org/linux/elrepo/el7/x86_64/RPMS/kmod-forcedeth-0.64-1.el7.elrepo.x86_64.rpm && \
depmod \${KVER}.el7.x86_64 && \
echo loop > /usr/lib/modules-load.d/loop.conf && \
echo 'options loop max_loop=8' > /usr/lib/modprobe.d/eightloop.conf && \
dracut -v -m 'anaconda base plymouth kernel-modules' --add-drivers forcedeth \
-f /initramfs/forcedeth.img --kver \${KVER}.el7.x86_64"
\end{lstlisting}

Tras ejecutar el comando anterior, aparecerá un nuevo subdirectorio en el directorio actual con el nombre \emph{initramfs}. Dentro se encuentra el \emph{initramfs} que acabamos de generar. Para listar su contenido podemos ejecutar \emph{`lsinitcpio initramfs/forcedeth.img'}.

\subsubsection{Subsistema Dracut}
Muchas distribuciones de Linux utilizan un único \emph{kernel} genérico con el que pretenden arrancar la más amplia variedad de hardware posible.
Los controladores de dispositivos para esta imagen genérica se incluyen como módulos de carga dinámica o \emph{`loadable modules'}. Se procede de esta manera ya que no es posible compilarlos todos de forma estática en el \emph{kernel} sin hacerlo demasiado grande como para arrancarlo desde sistemas con poca memoria.

Se plantea entonces el problema de detectar los dispositivos y cargar los módulos necesarios para montar el sistema de ficheros raíz durante el arranque o, para el caso, determinar dónde se encuentra y qué constituye el \emph{rootfs}.

La misión principal del subsitema \emph{Dracut} es montar el \emph{rootfs} detectando dispositivos y cargando los drivers que sean necesarios para ello.

En el siguiente esquema se representan las interacciones que se establecen entre \emph{Dracut} y el contenedor de datos \emph{data01}:\\

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.85]{bootstrap_2}
      \caption{Dracut}
\end{figure}

\section{Instalación}

\subsection{Anaconda}

\emph{Anaconda} es el programa de instalación usado por \emph{RedHat}, \emph{Fedora} y otras distribuciones de \emph{Linux}. Durante la instalación se detecta y configura el hardware del equipo y se prepara el sistema de ficheros adecuado para cada arquitectura. El instalador permite al usuario instalar y/o actualizar software en el equipo final. Una vez terminada la instalación, es necesario reiniciar el equipo para arrancar el nuevo sistema.

\emph{Anaconda} es un instalador muy completo. Permite instalar desde fuentes locales y remotas como CDs y DVDs o imágenes almacenadas en disco, \emph{NFS}, \emph{HTTP} o \emph{FTP}. Pero la característica más destacable de \emph{Anaconda} para este proyecto es su capacidad para automatizar las instalaciones mediante ficheros \emph{kickstart}.

\subsubsection{Kickstart}

La idea que hay detrás de los ficheros \emph{kickstart} es muy sencilla. Se trata de proporcionar al instalador \emph{Anaconda} un fichero con todas las respuestas a las preguntas que normalmente se hacen durante el proceso de instalación. El objetivo es que la instalación sea totalment desatendida.

Un fichero \emph{kickstart} no es más que un fichero de texto que contiene distintos elementos agrupados por secciones ordenadas. A continuación se muestra un fragmento del fichero \emph{kvm-akasa.ks} utilizado para aprovisionar los hipervisores de este proyecto:\\

\begin{lstlisting}[style=dnsmasq]
install
url --url="http://data01.demo.lan/centos/7/os/x86_64/"
text
keyboard es
lang en_US.UTF-8
eula --agreed
network --bootproto=dhcp --device=bootif --onboot=on
rootpw password
timezone Europe/Madrid --isUtc
services --disabled auditd,avahi-daemon,NetworkManager,postfix,microcode,tuned
services --enabled network,sshd
selinux --disabled
firewall --disabled
repo --name="CentOS" --baseurl=http://data01.demo.lan/centos/7/os/x86_64/
repo --name="Updates" --baseurl=http://data01.demo.lan/centos/7/updates/
repo --name="EPEL" --baseurl=http://data01.demo.lan/centos/7/epel/
repo --name="Misc" --baseurl=http://data01.demo.lan/centos/7/misc/
repo --name="Puppet-products" --baseurl=http://data01.demo.lan/puppet/puppetlabs-products/
repo --name="Puppet-deps" --baseurl=http://data01.demo.lan/puppet/puppetlabs-deps/
ignoredisk --only-use=sda
bootloader --location=mbr
zerombr
clearpart --all --initlabel
part swap --asprimary --fstype="swap" --size=1024
part /boot --fstype xfs --size=200
part / --fstype ext4 --size=1024 --grow
reboot
\end{lstlisting}

En la sección \emph{post} del fichero se le indica a \emph{Anaconda} que utilice \emph{R10K} para instalar el código que posteriormente será utilizado por \emph{Puppet}:\\

\begin{lstlisting}[style=dnsmasq]
% post --log=/root/ks-post-chroot.log
cat << EOF > /etc/r10k.yaml
cachedir: /var/cache/r10k
sources:
 puppet:
  remote: 'http://gito01.demo.lan/cgit/r10k-kvm'
   basedir: /etc/puppet/environments
EOF

rm -rf /etc/puppet
git clone http://gito01.demo.lan/cgit/puppet-config /etc/puppet
rm -rf /etc/puppet/environments/*
/usr/local/bin/r10k deploy environment
\end{lstlisting}

En el siguiente esquema se representan las interacciones que se establecen entre \emph{Anaconda}, el contenedor de datos \emph{data01} y el contenedor de repositorios de código \emph{gito01}:

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.85]{bootstrap_3}
      \caption{Anaconda}
\end{figure}

\subsection{R10K}

\emph{R10K} es una pequeña herramienta escrita en \emph{Ruby} que se ejecuta en el cliente y se encarga de gestionar módulos y entornos de \emph{Puppet}. En este trabajo los clientes son los equipos físicos \emph{kvm01} y \emph{kvm02} que ejercerán funciones de hipervisor. La configuración de \emph{R10K} se encuentra en un repositorio de control llamado \emph{r10k-kvm}.

El repositorio contiene un fichero de configuración llamado \emph{Puppetfile} y un fichero de datos en formato \emph{YAML}. El fichero de configuración contiene un listado de módulos de \emph{Puppet}, versiones y ubicaciones de descarga. El fichero de datos contiene parámetros que se le entregan al código para controlar su comportamiento. El cometido de \emph{R10K} es descargar el código y los datos para que \emph{Puppet} pueda entrar en acción.

A continuación vemos parte del contenido del fichero \emph{Puppetfile} usado por \emph{R10K} en los hipervisores para descargar el código de \emph{Puppet}:\\

\begin{lstlisting}[style=dnsmasq]
mod 'r_kvm',
  :git => 'http://gito01.demo.lan/cgit/puppet-r_kvm'

mod 'r_base',
  :git => 'http://gito01.demo.lan/cgit/puppet-r_base'

mod 'stdlib',
  :git => 'http://gito01.demo.lan/cgit/puppet-m_stdlib',
  :tag => '4.3.2'
\end{lstlisting}

El fichero de datos contiene parámetros que serán entregados al código en tiempo de compilación. A modo de muestra, el siguiente fragmento define algunas propiedades del super usuario de sistema \emph{root}:\\

\begin{lstlisting}[style=dnsmasq]
Users:
 root: &User_root
  linux:
   uid: '0'
   gid: '0'
   comment: 'root'
   pass: '!!'
   home: '/root'
   shell: '/bin/bash'
   keys:
    marc.villacorta:
     key: *SshKey_marc_villacorta
    deborah.aguilar:
     key: *SshKey_deborah_aguilar
   profiles:
    - 'root'
    - 'marc.villacorta'
    - 'deborah.aguilar'
\end{lstlisting}

\subsection{Puppet}

\emph{Puppet} es una herramienta de gestión de configuraciones que incorpora un lenguaje declarativo específicamente diseñado para este cometido. Suele usarse de forma centralizada gracias a su arquitectura cliente-servidor.

El usuario utiliza el lenguaje para describir el estado deseado para su sistema cliente. Al código resultante se le llama \emph{'manifest'} y se almacena en el servidor. Los clientes toman la iniciativa y solicitan su configuración al servidor cada cierto tiempo.

En la solicitud del cliente se incorpora un informe de hechos o \emph{'facts'}. Este informe no es más que un listado de pares de clave y valor con datos descriptivos del sistema cliente.

Veamos un ejemplo de \emph{facter}, la herramienta que utiliza el cliente para generar sus \emph{facts}:\\

\begin{lstlisting}[style=dnsmasq]
[root@kvm01 ~]# facter architecture bios_vendor fqdn ipaddress operatingsystem kernel
architecture => x86_64
bios_vendor => Intel Corp.
fqdn => kvm01.demo.lan
ipaddress => 192.168.1.113
kernel => Linux
operatingsystem => CentOS
\end{lstlisting}

El servidor, que ahora se encuentra en posesión del \emph{manifest} y de los \emph{facts}, interpola los valores y compila el código produciendo, de esta manera, el catálogo o \emph{'catalog'} que se entrega al cliente.

En este punto el cliente realiza una introspección para obtener la visión de cual es su estado actual. El estado actual del sistema se compara con el estado deseado del sistema (definido en el catálogo) y \emph{Puppet} determina y ejecuta las transiciones que deben llevarse a cabo para pasar de un estado al otro.

En este proyecto no se utiliza la arquitectura cliente-servidor de \emph{Puppet}. Todo el procedimiento descrito se realiza en el cliente de forma totalmente descentralizada. Esta arquitectura en la que no existe servidor, se conoce también con el nombre de \emph{Masterless}. El \emph{manifest} de cada cliente se sincroniza con \emph{R10K} que lo recupera de un servidor \emph{gitolite} de repositorios \emph{Git}.

En el siguiente esquema se pueden apreciar las relaciones que se establecen entre el servidor que queremos aprovisionar \emph{kvm01} y los contenedores \emph{gito01}, \emph{data01} y \emph{regi01}:\\

\begin{figure}[h]
  \centering
    \includegraphics[scale=0.85]{bootstrap_4}
      \caption{R10K, Puppet y CoreOS}
\end{figure}

%------------------------
% Guía de inicio rápido:
%------------------------

\chapter{Guía de inicio rápido}

En este último capítulo veremos paso a paso como instalar la plataforma de arranque en un sistema Linux. Al finalizar la instalación, nuestro sistema estará listo para desplegar la plataforma de explotación.

La instalación de la plataforma de arranque despliega la siguiente estructura de ficheros y directorios en el sistema de ficheros del equipo local:\\

\begin{lstlisting}[style=dnsmasq,extendedchars=\true,literate={ó}{{\'o}}1]
data <------------------------ Directorios de datos persistentes montados por los contenedores.
  boot/
  data/
  gito/
  regi/
etc
  booddies <------------------ Ficheros de configuración de cada contenedor.
    boot.conf
    cgit.conf
    data.conf
    gito.conf
    regi.conf
  systemd
    system <------------------ Ficheros de configuración para los servicios de systemd.
      boot.service
      cgit.service
      data.service
      gito.service
      regi.service
usr
  local
    sbin <-------------------- Lógica de ejecución.
      runctl-boot
      runctl-cgit
      runctl-data
      runctl-gito
      runctl-regi
\end{lstlisting}

La primera vez que se arrancan los servicios, se descargan las imágenes de los seis contenedores de forma automática:\\

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{C0C0C0}
\textbf{Imagen} & \textbf{ID} & \textbf{Configuración} & \textbf{Systemd} & \textbf{Lógica} \\ \hline
h0tbird/boot    & boot01      & boot.conf              & boot.service     & runctl-boot     \\ \hline
h0tbird/data    & data01      & data.conf              & data.service     & runctl-data     \\ \hline
h0tbird/gito    & gito01      & gito.conf              & gito.service     & runctl-gito     \\ \hline
h0tbird/cgit    & cgit01      & cgit.conf              & cgit.service     & runctl-cgit     \\ \hline
h0tbird/regi    & regi01      & regi.conf              & regi.service     & runctl-regi     \\ \hline
h0tbird/ntpd    & ntpd01      & ntpd.conf              & ntpd.service     & runctl-ntpd     \\ \hline
\end{tabular}
\caption{Imágenes de los contenedores}
\end{table}

\clearpage

\section{Requisitos}

Para instalar la plataforma de arranque necesitaremos:

\begin{itemize}
  \item Un sistema Linux que disponga de \emph{'systemd'}.
  \item \emph{Docker} instalado y arrancado con la opción \emph{'--insecure-registry=regi01:5000'}
  \item Una interfaz física de red conectada al bridge \emph{br0}.
  \item Unos 20GB de espacio libre en \emph{/data}.
\end{itemize}

\section{Primer paso: Instalar}

\subsection{Clonar e instalar}

Es necesario realizar un clonado recursivo del repositorio \emph{Git} para descargar todos los submódulos contenidos en el mismo:\\

\begin{lstlisting}[style=dnsmasq]
# git clone --recursive https://github.com/h0tbird/booddies.git
\end{lstlisting}

A continuación, la ejecución del siguiente comando proporcionará la estructura de ficheros y directorios detallada anteriormente:\\

\begin{lstlisting}[style=dnsmasq]
# cd booddies && sudo ./bin/install

:: Install /etc/systemd/system/boot.service           [DONE]
:: Install /usr/local/sbin/runctl-boot                [DONE]
:: Install /etc/booddies/boot.conf                    [DONE]
:: Install /etc/systemd/system/cgit.service           [DONE]
:: Install /usr/local/sbin/runctl-cgit                [DONE]
:: Install /etc/booddies/cgit.conf                    [DONE]
:: Install /etc/systemd/system/data.service           [DONE]
:: Install /usr/local/sbin/runctl-data                [DONE]
:: Install /etc/booddies/data.conf                    [DONE]
:: Install /etc/systemd/system/gito.service           [DONE]
:: Install /usr/local/sbin/runctl-gito                [DONE]
:: Install /etc/booddies/gito.conf                    [DONE]
:: Install /etc/systemd/system/regi.service           [DONE]
:: Install /usr/local/sbin/runctl-regi                [DONE]
:: Install /etc/booddies/regi.conf                    [DONE]
:: Run 'systemctl daemon-reload'                      [DONE]
\end{lstlisting}

\subsection{Configurar los servicios}

Es necesario editar todos los ficheros de configuración que hay en el directorio \emph{'/etc/booddies'} para adaptarlos a nuestra configuración de red y \emph{DNS}.

En el fichero de configuración \emph{'/etc/booddies/boot.conf'} podemos definir parámetros relativos a los servicios \emph{DHCP} y \emph{DNS}:\\

\begin{lstlisting}[style=dnsmasq]
# cat /etc/booddies/boot.conf

readonly ID='boot01'
readonly IMAGE='h0tbird/boot:latest'
readonly HOSTNAME='boot01.demo.lan'
readonly DATA_DIR='/data/boot'
readonly DHCP_RANGE='192.168.1.50,192.168.1.150,12h'
readonly DHCP_OPTION='option:router,192.168.1.1'
readonly DNS_UPSTREAMS='8.8.8.8 8.8.4.4'
readonly DOMAIN='demo.lan'
readonly IP='192.168.1.2'
readonly PREFIX='24'
readonly GATEWAY='192.168.1.1'
\end{lstlisting}

El contenedor \emph{gito} recibe un par de claves \emph{SSH} (pública y privada). Estas claves permiten recibir y propagar cambios de los repositorios privados a los públicos. Los repositorios públicos se encuentran ubicados en \emph{GitHub}.

Si alguien quisiera realizar contribuciones al proyecto tendría que crear un \emph{'fork'} del mismo en \emph{GitHub} y generar y autorizar su propio par de claves \emph{SSH}:\\

\begin{lstlisting}[style=dnsmasq]
# cat /etc/booddies/gito.conf

readonly ID='gito01'
readonly IMAGE='h0tbird/gito:latest'
readonly HOSTNAME='gito01.demo.lan'
readonly DATA_DIR='/data/gito'
readonly SSH_PRI_KEY='/home/marc/.ssh/gitolite.key'
readonly SSH_PUB_KEY='/home/marc/.ssh/gitolite.key.pub'
readonly TRUST_HOSTS='github.com'
\end{lstlisting}

En \emph{'/etc/booddies/data.conf'} configuramos la variable \emph{'COREOS-CHANNEL'} que define el canal de distribución de \emph{CoreOS} al que nos queremos subscribir. Existen tres canales, \emph{Stable}, \emph{Beta} y \emph{Alpha}:\\

\begin{lstlisting}[style=dnsmasq]
# cat /etc/booddies/data.conf

readonly ID='data01'
readonly IMAGE='h0tbird/data:latest'
readonly HOSTNAME='data01.demo.lan'
readonly DATA_DIR='/data/data'
readonly COREOS_CHANNEL='beta'
\end{lstlisting}

El resto de ficheros de configuración que hay en \emph{'/etc/booddies'} no tienen mayor complicación.

\clearpage

\subsection{Arrancar los servicios}

La primera vez que se arrancan los servicios se descargan las imágenes del registro públic en \emph{Docker Hub}:\\

\begin{lstlisting}[style=dnsmasq]
# sudo systemctl start boot data gito cgit regi ntpd
\end{lstlisting}

Podemos monitorizar el proceso de la siguiente manera:\\

\begin{lstlisting}[style=dnsmasq]
# journalctl -f -u boot -u data -u gito -u cgit -u regi -u ntpd
\end{lstlisting}

\section{Segundo paso: Sincronizar los datos}

Descargar todos los datos ahora permite a la plataforma de arranque operar de manera totalmente independiente de internet. Durante todo el proceso de sincronización se puede consultar el fichero \emph{'/tmp/booddies.log'} para ver los detalles sobre el progreso.

\subsection{Sincronizar los repositorios YUM}

En este paso se van a descargar unos 15GB de datos:\\

\begin{lstlisting}[style=dnsmasq]
# ./bin/feed-data

:: Synchronizing 'base' data...                       [DONE]
:: Synchronizing 'updates' data...                    [DONE]
:: Synchronizing 'puppetlabs-products' data...        [DONE]
:: Synchronizing 'puppetlabs-deps' data...            [DONE]
:: Synchronizing 'epel' data...                       [DONE]
:: Synchronizing 'misc' data...                       [DONE]
:: Synchronizing 'coreos' data...                     [DONE]
\end{lstlisting}

\subsection{Sincronizar el kernel y el initrd}

De los datos descargados en el paso anterior, realizamos una copia tanto del \emph{kernel} como del \emph{initrd}. Esta copia será la que el contenedor \emph{boot} servirá vía \emph{PXE}:\\

\begin{lstlisting}[style=dnsmasq]
# ./bin/feed-boot

:: Synchronizing 'vmlinuz' data...                    [DONE]
:: Synchronizing 'initrd' data...                     [DONE]
\end{lstlisting}

\subsection{Sincronizar el registro privado}

En este paso descargamos imágenes de contenedores que están publicadas en el registro público de \emph{Docker Hub} y las almacenamos en el registro privado \emph{regi}:\\

\begin{lstlisting}[style=dnsmasq]
:: Seeking 'zookeeper/3.4.6' private image...                               [DONE]
:: Seeking 'mesos-master/0.22.1-1.0.ubuntu1404' private image...            [DONE]
:: Seeking 'mesos-slave/0.22.1-1.0.ubuntu1404' private image...             [DONE]
:: Seeking 'marathon/v0.8.1' private image...                               [DONE]
\end{lstlisting}

\subsection{Sincronizar los repositorios de código}

Terminamos la sincronización importando los repositorios de código que contienen las recetas de \emph{Puppet}. Estas recetas o \emph{'manifests'} se utilizarán principalmente para configurar el hipervisor y para preparar el contexto necesario para el arranque de \emph{CoreOS}.\\

\begin{lstlisting}[style=dnsmasq]
# ./bin/feed-gito

:: Synchronizing 'https://github.com/h0tbird/puppet-p_ssh.git'...                  [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-m_terminfo.git'...             [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-r_kvm.git'...                  [DONE]
:: Synchronizing 'https://github.com/puppetlabs/puppetlabs-stdlib.git'...          [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-m_misclib.git'...              [DONE]
:: Synchronizing 'https://github.com/h0tbird/r10k-kvm.git'...                      [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-m_users.git'...                [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-m_ssh.git'...                  [DONE]
:: Synchronizing 'https://github.com/h0tbird/coreos-mesos.git'...                  [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-r_base.git'...                 [DONE]
:: Synchronizing 'https://github.com/h0tbird/puppet-config.git'...                 [DONE]
\end{lstlisting}

\section{Tercer paso: Configurar}

El último paso antes de arrancar los equipos físicos vía \emph{PXE}, consiste en realizar las adaptaciones necesarias sobre los ficheros de arranque de \emph{'pxelinux'} y los ficheros de instalación de \emph{'kickstart'}.

La ubicación de estos ficheros es la siguiente:\\

\begin{lstlisting}[style=dnsmasq]
# ll -d /data/{boot/pxelinux,data/kickstarts}

drwxr-xr-x 2 root root 4.0K Apr 20 19:59 /data/boot/pxelinux/
drwxr-xr-x 3 root root 4.0K Apr 12 11:36 /data/data/kickstarts/
\end{lstlisting}

A modo de referencia, se pueden utilizar los ficheros de ejemplo que hay en el repositorio. Estos ficheros son válidos para los sistemas que se han usado para montar la maqueta. Incluso es posible que se puedan reutilizar para otros equipos sin modificación alguna.

%------------
% Biografía:
%------------

\begin{thebibliography}{2}

%% Llibres:  Autor/s (cognoms i inicials dels noms), títol del llibre (en cursiva), editor, ciutat i any de publicació. Quan es cita el capítol d'un llibre s'ha d'indicar el títol del capítol (entre cometes), el títol del llibre (en cursiva) i els números de pàgines amb la primera i la darrera incloses.

\bibitem{item1}
Netcraft: April 2015 Web Server Survey (http://news.netcraft.com/archives/2015/04/20/april-2015-web-server-survey.html)

\bibitem{item2}
Google data center locations (http://www.google.com/about/datacenters/inside/locations/index.html)

\bibitem{item3}
McKinsey \& Company ``Revolutionizing data center efficiency''. {\it Uptime Institute Symposium}
(2008)

\bibitem{item4}
Gartner says efficient data center design can lead to 300 percent capacity growth in 60 percent less space. (http://www.gartner.com/newsroom/id/1472714)

\bibitem{item5}
Arunchandar Vasan / Anand Sivasubramaniam / Vikrant Shimpi / T. Sivabalan / Rajesh Subbiah. ``Worth their watts? an empirical study of datacenter servers.'' {\it In Proc. of the 16th International Symposium on High Performance Computer Architecture (HPCA). Bangalore, India} (2010)

\bibitem{item6}
Schwarzkopf, M. / Konwinski, A. / Abd-El-Malek, M. / Wilkes, J.
``Omega: flexible, scalable schedulers for large compute clusters''. {\it EuroSys}.
(2013)

\bibitem{item7}
Hindman, B. / Konwinski, A. / Zaharia, M. / Ghodsi, A. / Joseph, A. / Katz, R. / Shenker, S. / Stoica, I.
``Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center''. {\it U.C. Berkeley}.
(2013)

\bibitem{item8}
Delimitrou, C. / Kozyrakis, C.
``Quasar: Resource-Efficient and QoS-Aware Cluster Management''. {\it Stanford University}.
(2014)

\bibitem{item9}
Turnbull, J. / McCune, J.
{\it Pro Puppet}.
(Apress. New York. 2011)

\bibitem{item10}
Turnbull, J.
{\it The Docker Book}.
(2014)

\end{thebibliography}
\end{document}
